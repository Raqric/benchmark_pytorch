{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "computational-booking",
   "metadata": {
    "papermill": {
     "duration": 0.009225,
     "end_time": "2021-04-13T15:53:31.684442",
     "exception": false,
     "start_time": "2021-04-13T15:53:31.675217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Benchmarking GPUs in Kaggle\n",
    "\n",
    "In this Kaggle notebook there are two examples of benchmarks GPU with pytorch. On one side it uses a model performance and on the other side it is an adaption from my previous CPU benchmark. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-cigarette",
   "metadata": {
    "papermill": {
     "duration": 0.007753,
     "end_time": "2021-04-13T15:53:31.700267",
     "exception": false,
     "start_time": "2021-04-13T15:53:31.692514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Benchmark GPU With models\n",
    "Code from project: https://github.com/ryujaehun/pytorch-gpu-benchmark\n",
    "\n",
    "Recollecting performance data of Kaggle.  **It must be activated the GPU accelerator option.**\n",
    "\n",
    "The next block of code executes an extensive benchmark with DataSets from PyTorch on the GPU among differents types (type float, half and double). We are just going to use one type to simplify executions and tests.\n",
    "\n",
    "By default this part is not going to be executed with \"Run All\" option (it has a long time execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "induced-syndrome",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-13T15:53:31.721369Z",
     "iopub.status.busy": "2021-04-13T15:53:31.719830Z",
     "iopub.status.idle": "2021-04-13T15:53:31.772723Z",
     "shell.execute_reply": "2021-04-13T15:53:31.773137Z"
    },
    "papermill": {
     "duration": 0.065208,
     "end_time": "2021-04-13T15:53:31.773359",
     "exception": false,
     "start_time": "2021-04-13T15:53:31.708151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%script` not found.\n"
     ]
    }
   ],
   "source": [
    "#Comment the line after to execute correctly the block code and in the rest of blocks code\n",
    "%%script false --no-raise-error\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import platform\n",
    "import psutil\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936\n",
    "# This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware. \n",
    "# If you check it using the profile tool, the cnn method such as winograd, fft, etc. is used for the first iteration and the best operation is selected for the device.\n",
    "\n",
    "\n",
    "\n",
    "MODEL_LIST = {\n",
    "\n",
    "    models.mnasnet:models.mnasnet.__all__[1:],\n",
    "    models.resnet: models.resnet.__all__[1:],\n",
    "    models.densenet: models.densenet.__all__[1:],\n",
    "    models.squeezenet: models.squeezenet.__all__[1:],\n",
    "    models.vgg: models.vgg.__all__[1:],\n",
    "    models.mobilenet:models.mobilenet.__all__[1:],\n",
    "    models.shufflenetv2:models.shufflenetv2.__all__[1:]\n",
    "}\n",
    "#ORIGINAL\n",
    "#precisions=[\"float\",\"half\",'double']\n",
    "precisions=[\"float\"]\n",
    "\n",
    "# For post-voltaic architectures, there is a possibility to use tensor-core at half precision.\n",
    "# Due to the gradient overflow problem, apex is recommended for practical use.\n",
    "device_name =str(torch.cuda.get_device_name(0))\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Benchmarking')\n",
    "parser.add_argument('--WARM_UP','-w', type=int,default=5, required=False, help=\"Num of warm up\")\n",
    "parser.add_argument('--NUM_TEST','-n', type=int,default=50,required=False, help=\"Num of Test\")\n",
    "parser.add_argument('--BATCH_SIZE','-b', type=int, default=12, required=False, help='Num of batch size')\n",
    "parser.add_argument('--NUM_CLASSES','-c', type=int, default=1000, required=False, help='Num of class')\n",
    "parser.add_argument('--NUM_GPU','-g', type=int, default=1, required=False, help='Num of gpus')\n",
    "parser.add_argument('--folder','-f', type=str, default='result', required=False, help='folder to save results')\n",
    "args = parser.parse_args()\n",
    "args.BATCH_SIZE*=args.NUM_GPU\n",
    "class RandomDataset(Dataset):\n",
    "\n",
    "    def __init__(self,  length):\n",
    "        self.len = length\n",
    "        self.data = torch.randn( 3, 224, 224,length)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[:,:,:,index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "rand_loader = DataLoader(dataset=RandomDataset( args.BATCH_SIZE*(args.WARM_UP + args.NUM_TEST)),\n",
    "                         batch_size=args.BATCH_SIZE, shuffle=False,num_workers=8)\n",
    "def train(precision='single'):\n",
    "    \"\"\"use fake image for training speed test\"\"\"\n",
    "    target = torch.LongTensor(args.BATCH_SIZE).random_(args.NUM_CLASSES).cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    benchmark = {}\n",
    "    for model_type in MODEL_LIST.keys():\n",
    "        for model_name in MODEL_LIST[model_type]:\n",
    "            model = getattr(model_type, model_name)(pretrained=False)\n",
    "            if args.NUM_GPU > 1:\n",
    "                model = nn.DataParallel(model,device_ids=range(args.NUM_GPU))\n",
    "            model=getattr(model,precision)()\n",
    "            model=model.to('cuda')\n",
    "            durations = []\n",
    "            print(f'Benchmarking Training {precision} precision type {model_name} ')\n",
    "            for step,img in enumerate(rand_loader):\n",
    "                img=getattr(img,precision)()\n",
    "                torch.cuda.synchronize()\n",
    "                start = time.time()\n",
    "                model.zero_grad()\n",
    "                prediction = model(img.to('cuda'))\n",
    "                loss = criterion(prediction, target)\n",
    "                loss.backward()\n",
    "                torch.cuda.synchronize()\n",
    "                end = time.time()\n",
    "                if step >= args.WARM_UP:\n",
    "                    durations.append((end - start)*1000)\n",
    "            print(f'{model_name} model average train time : {sum(durations)/len(durations)}ms')\n",
    "            del model\n",
    "            benchmark[model_name] = durations\n",
    "    return benchmark\n",
    "\n",
    "def inference(precision='float'):\n",
    "    benchmark = {}\n",
    "    with torch.no_grad():\n",
    "        for model_type in MODEL_LIST.keys():\n",
    "            for model_name in MODEL_LIST[model_type]:\n",
    "                model = getattr(model_type, model_name)(pretrained=False)\n",
    "                if args.NUM_GPU > 1:\n",
    "                    model = nn.DataParallel(model,device_ids=range(args.NUM_GPU))\n",
    "                model=getattr(model,precision)()\n",
    "                model=model.to('cuda')\n",
    "                model.eval()\n",
    "                durations = []\n",
    "                print(f'Benchmarking Inference {precision} precision type {model_name} ')\n",
    "                for step,img in enumerate(rand_loader):\n",
    "                    img=getattr(img,precision)()\n",
    "                    torch.cuda.synchronize()\n",
    "                    start = time.time()\n",
    "                    model(img.to('cuda'))\n",
    "                    torch.cuda.synchronize()\n",
    "                    end = time.time()\n",
    "                    if step >= args.WARM_UP:\n",
    "                        durations.append((end - start)*1000)\n",
    "                print(f'{model_name} model average inference time : {sum(durations)/len(durations)}ms')\n",
    "                del model\n",
    "                benchmark[model_name] = durations\n",
    "    return benchmark\n",
    "\n",
    "f\"{platform.uname()}\\n{psutil.cpu_freq()}\\ncpu_count: {psutil.cpu_count()}\\nmemory_available: {psutil.virtual_memory().available}\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    folder_name= \"../output/results\"\n",
    "    \n",
    "    device_name=f\"{device_name}_{args.NUM_GPU}_gpus_\"\n",
    "    system_configs=f\"{platform.uname()}\\n\\\n",
    "                     {psutil.cpu_freq()}\\n\\\n",
    "                    cpu_count: {psutil.cpu_count()}\\n\\\n",
    "                    memory_available: {psutil.virtual_memory().available}\"\n",
    "    gpu_configs=[torch.cuda.device_count(),torch.version.cuda,torch.backends.cudnn.version(),torch.cuda.get_device_name(0)]\n",
    "    gpu_configs=list(map(str,gpu_configs))\n",
    "    temp=['Number of GPUs on current device : ','CUDA Version : ','Cudnn Version : ','Device Name : ']\n",
    "\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    with open(os.path.join(folder_name, 'config.json'), 'w') as f:\n",
    "        json.dump(vars(args), f, indent=2)\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    start_time=now.strftime('%Y/%m/%d %H:%M:%S')\n",
    "    \n",
    "    print(f'benchmark start : {start_time}')\n",
    "\n",
    "    for idx,value in enumerate(zip(temp,gpu_configs)):\n",
    "        gpu_configs[idx]=''.join(value)\n",
    "        print(gpu_configs[idx])\n",
    "    print(system_configs)\n",
    "\n",
    "    with open(os.path.join(folder_name,\"system_info.txt\"), \"w\") as f:\n",
    "        f.writelines(f'benchmark start : {start_time}\\n')\n",
    "        f.writelines('system_configs\\n\\n')\n",
    "        f.writelines(system_configs)\n",
    "        f.writelines('\\ngpu_configs\\n\\n')\n",
    "        f.writelines(s + '\\n' for s in gpu_configs )\n",
    "\n",
    "    \n",
    "    for precision in precisions:\n",
    "        train_result=train(precision)\n",
    "        train_result_df = pd.DataFrame(train_result)\n",
    "        path=f'{folder_name}/{device_name}_{precision}_model_train_benchmark.csv'\n",
    "        train_result_df.to_csv(path, index=False)\n",
    "\n",
    "        inference_result=inference(precision)\n",
    "        inference_result_df = pd.DataFrame(inference_result)\n",
    "        path=f'{folder_name}/{device_name}_{precision}_model_inference_benchmark.csv'\n",
    "        inference_result_df.to_csv(path, index=False)\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    end_time=now.strftime('%Y/%m/%d %H:%M:%S')\n",
    "    print(f'benchmark end : {end_time}')\n",
    "    with open(os.path.join(folder_name,\"system_info.txt\"), \"a\") as f:\n",
    "        f.writelines(f'benchmark end : {end_time}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "contained-runner",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-13T15:53:31.798848Z",
     "iopub.status.busy": "2021-04-13T15:53:31.798300Z",
     "iopub.status.idle": "2021-04-13T15:53:31.807246Z",
     "shell.execute_reply": "2021-04-13T15:53:31.806560Z"
    },
    "papermill": {
     "duration": 0.025271,
     "end_time": "2021-04-13T15:53:31.807349",
     "exception": false,
     "start_time": "2021-04-13T15:53:31.782078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "import pandas as pd\n",
    "import os,time\n",
    "import glob\n",
    "import cufflinks as cf\n",
    "import plotly.offline\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=True, world_readable=True)\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "answering-factor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-13T15:53:31.830563Z",
     "iopub.status.busy": "2021-04-13T15:53:31.828073Z",
     "iopub.status.idle": "2021-04-13T15:53:31.837916Z",
     "shell.execute_reply": "2021-04-13T15:53:31.837427Z"
    },
    "papermill": {
     "duration": 0.022068,
     "end_time": "2021-04-13T15:53:31.838018",
     "exception": false,
     "start_time": "2021-04-13T15:53:31.815950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "MODEL_LIST = {\n",
    "    'mnasnet':models.mnasnet.__all__[1:],\n",
    "    'resnet': models.resnet.__all__[1:],\n",
    "    'densenet': models.densenet.__all__[1:],\n",
    "    'squeezenet': models.squeezenet.__all__[1:],\n",
    "    'vgg': models.vgg.__all__[1:],\n",
    "    'mobilenet':models.mobilenet.__all__[1:],\n",
    "    'shufflenetv2':models.shufflenetv2.__all__[1:]\n",
    "}\n",
    "folder_name='../output/results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fluid-ghana",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-13T15:53:31.859495Z",
     "iopub.status.busy": "2021-04-13T15:53:31.858748Z",
     "iopub.status.idle": "2021-04-13T15:53:31.867803Z",
     "shell.execute_reply": "2021-04-13T15:53:31.866978Z"
    },
    "papermill": {
     "duration": 0.021154,
     "end_time": "2021-04-13T15:53:31.867923",
     "exception": false,
     "start_time": "2021-04-13T15:53:31.846769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "csv_list=glob.glob(folder_name+'/*.csv')\n",
    "columes=[]\n",
    "for key,values in MODEL_LIST.items():\n",
    "    for i in values:\n",
    "        columes.append((key,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opposite-defensive",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-13T15:53:31.889701Z",
     "iopub.status.busy": "2021-04-13T15:53:31.888947Z",
     "iopub.status.idle": "2021-04-13T15:53:31.898278Z",
     "shell.execute_reply": "2021-04-13T15:53:31.897426Z"
    },
    "papermill": {
     "duration": 0.021794,
     "end_time": "2021-04-13T15:53:31.898392",
     "exception": false,
     "start_time": "2021-04-13T15:53:31.876598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "for csv in csv_list:\n",
    "    df=pd.read_csv(csv)\n",
    "    df.columns = pd.MultiIndex.from_tuples(columes)\n",
    "    df.groupby(level=0,axis=1).mean().mean()\n",
    "#     print(csv)\n",
    "    title=csv.split('/')[1].split('_benchmark')[0]\n",
    "    title=title.replace(' ','_')\n",
    "    df.groupby(level=0,axis=1).mean().mean().iplot(kind='scatter',mode='markers',title=title,yTitle='time(ms)',xTitle='models',asImage=True,filename=title)\n",
    "    for model in MODEL_LIST.keys():\n",
    "        df.mean()[model].iplot(kind='scatter',mode='markers',title=model+\"_\"+title,yTitle='time(ms)',xTitle='models',asImage=True,filename=model+\"_\"+title)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-tractor",
   "metadata": {
    "papermill": {
     "duration": 0.008446,
     "end_time": "2021-04-13T15:53:31.915768",
     "exception": false,
     "start_time": "2021-04-13T15:53:31.907322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## My CPU Benchmark adapted to GPU\n",
    "\n",
    "Pytorch has hard coded a block size of 512 threads. So there's only one execution per matrix size.\n",
    "\n",
    "As last benchmark, GPU accelerator has to be activated to use this benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "later-seeker",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-13T15:53:31.937712Z",
     "iopub.status.busy": "2021-04-13T15:53:31.937099Z",
     "iopub.status.idle": "2021-04-13T15:53:33.846280Z",
     "shell.execute_reply": "2021-04-13T15:53:33.845698Z"
    },
    "papermill": {
     "duration": 1.922023,
     "end_time": "2021-04-13T15:53:33.846411",
     "exception": false,
     "start_time": "2021-04-13T15:53:31.924388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "#Author: Raquel Ricoy\n",
    "\n",
    "#Benchmark to study Kaggle's GPUs, CPUs and TPUs potential.\n",
    "#It's going to use Pytorch and to stablish a script to calculate its performance and GFLOPS.\n",
    "\n",
    "#Install pytorch\n",
    "#!conda install -y pytorch torchvision -c pytorch\n",
    "\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import os\n",
    "#print(os.listdir(\"../input\"))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "#Importing Libraries needed for use torch\n",
    "import timeit\n",
    "import torch.utils.benchmark as benchmark\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "decreased-diamond",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-13T15:53:33.870377Z",
     "iopub.status.busy": "2021-04-13T15:53:33.869697Z",
     "iopub.status.idle": "2021-04-13T15:53:33.872272Z",
     "shell.execute_reply": "2021-04-13T15:53:33.872719Z"
    },
    "papermill": {
     "duration": 0.017229,
     "end_time": "2021-04-13T15:53:33.872840",
     "exception": false,
     "start_time": "2021-04-13T15:53:33.855611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Functions obtained from Torch Webpages por PyTorch Benchmarks\n",
    "def batched_dot_mul_sum(a, b):\n",
    "    '''Computes batched dot by multiplying and summing'''\n",
    "    return a.mul(b).sum(-1)\n",
    "\n",
    "\n",
    "def batched_dot_bmm(a, b):\n",
    "    '''Computes batched dot by reducing to bmm'''\n",
    "    a = a.reshape(-1, 1, a.shape[-1])\n",
    "    b = b.reshape(-1, b.shape[-1], 1)\n",
    "    return torch.bmm(a, b).flatten(-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "military-astronomy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-13T15:53:33.899460Z",
     "iopub.status.busy": "2021-04-13T15:53:33.898772Z",
     "iopub.status.idle": "2021-04-13T15:53:33.901099Z",
     "shell.execute_reply": "2021-04-13T15:53:33.901596Z"
    },
    "papermill": {
     "duration": 0.020027,
     "end_time": "2021-04-13T15:53:33.901724",
     "exception": false,
     "start_time": "2021-04-13T15:53:33.881697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Method that do the benchmark and compare results with dot mul sum implementations and vectorSum\n",
    "def benchMark(sizes,nThreads):\n",
    "    results = []\n",
    "    if(len(sizes) == 0):\n",
    "        print(\"Parameter 'sizes' has to a have minumun of 1 parameters\")\n",
    "        return\n",
    "    if(len(nThreads)==0):\n",
    "        print(\"Parameter 'nThreads' has to a have minumun of 1 parameters\")\n",
    "    \n",
    "    for n in sizes:\n",
    "        # label and sub_label are the rows\n",
    "        # description is the column\n",
    "        label = 'Batched dot'\n",
    "        sub_label = f'[{n}, {n}]'\n",
    "        x = torch.ones((n, n),device='cuda')\n",
    "        for num_threads in nThreads:\n",
    "            results.append(benchmark.Timer(\n",
    "                stmt='batched_dot_mul_sum(x, x)',\n",
    "                setup='from __main__ import batched_dot_mul_sum',\n",
    "                globals={'x': x},\n",
    "                num_threads=num_threads,\n",
    "                label=label,\n",
    "                sub_label=sub_label,\n",
    "                description='mul/sum',\n",
    "            ).blocked_autorange(min_run_time=1))\n",
    "            results.append(benchmark.Timer(\n",
    "                stmt='batched_dot_bmm(x, x)',\n",
    "                setup='from __main__ import batched_dot_bmm',\n",
    "                globals={'x': x},\n",
    "                num_threads=num_threads,\n",
    "                label=label,\n",
    "                sub_label=sub_label,\n",
    "                description='bmm',\n",
    "            ).blocked_autorange(min_run_time=1))\n",
    "    compare = benchmark.Compare(results)\n",
    "    compare.print()\n",
    "    return compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "amino-microwave",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-13T15:53:33.930689Z",
     "iopub.status.busy": "2021-04-13T15:53:33.930121Z",
     "iopub.status.idle": "2021-04-13T15:55:11.807961Z",
     "shell.execute_reply": "2021-04-13T15:55:11.808432Z"
    },
    "papermill": {
     "duration": 97.898002,
     "end_time": "2021-04-13T15:55:11.808620",
     "exception": false,
     "start_time": "2021-04-13T15:53:33.910618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark execution:  1 \n",
      "\n",
      "[-------------- Batched dot ---------------]\n",
      "                      |  mul/sum  |    bmm  \n",
      "1 threads: ---------------------------------\n",
      "      [512, 512]      |     20.8  |     34.6\n",
      "      [2048, 2048]    |     99.5  |     72.1\n",
      "      [4096, 4096]    |    377.0  |    268.0\n",
      "      [8192, 8192]    |   1454.3  |    994.9\n",
      "      [16384, 16384]  |   5753.8  |   3851.4\n",
      "      [32768, 32768]  |  22982.4  |  15212.9\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "Benchmark execution:  2 \n",
      "\n",
      "[-------------- Batched dot ---------------]\n",
      "                      |  mul/sum  |    bmm  \n",
      "1 threads: ---------------------------------\n",
      "      [512, 512]      |     20.8  |     22.1\n",
      "      [2048, 2048]    |     99.5  |     72.0\n",
      "      [4096, 4096]    |    377.0  |    268.0\n",
      "      [8192, 8192]    |   1454.2  |    994.3\n",
      "      [16384, 16384]  |   5754.0  |   3851.2\n",
      "      [32768, 32768]  |  22984.8  |  15215.8\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "Benchmark execution:  3 \n",
      "\n",
      "[-------------- Batched dot ---------------]\n",
      "                      |  mul/sum  |    bmm  \n",
      "1 threads: ---------------------------------\n",
      "      [512, 512]      |     20.6  |     25.6\n",
      "      [2048, 2048]    |     99.6  |     72.0\n",
      "      [4096, 4096]    |    376.9  |    268.0\n",
      "      [8192, 8192]    |   1454.3  |    995.0\n",
      "      [16384, 16384]  |   5753.9  |   3851.8\n",
      "      [32768, 32768]  |  22982.7  |  15212.9\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "Benchmark execution:  4 \n",
      "\n",
      "[-------------- Batched dot ---------------]\n",
      "                      |  mul/sum  |    bmm  \n",
      "1 threads: ---------------------------------\n",
      "      [512, 512]      |     20.8  |     21.5\n",
      "      [2048, 2048]    |     99.6  |     72.0\n",
      "      [4096, 4096]    |    377.0  |    267.9\n",
      "      [8192, 8192]    |   1454.2  |    995.1\n",
      "      [16384, 16384]  |   5753.9  |   3851.0\n",
      "      [32768, 32768]  |  22980.4  |  15213.5\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "Benchmark execution:  5 \n",
      "\n",
      "[-------------- Batched dot ---------------]\n",
      "                      |  mul/sum  |    bmm  \n",
      "1 threads: ---------------------------------\n",
      "      [512, 512]      |     21.8  |     21.6\n",
      "      [2048, 2048]    |     99.6  |     72.0\n",
      "      [4096, 4096]    |    377.1  |    267.9\n",
      "      [8192, 8192]    |   1454.0  |    995.3\n",
      "      [16384, 16384]  |   5753.9  |   3851.2\n",
      "      [32768, 32768]  |  22981.0  |  15211.5\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The limit dimension of the sizes is [100000,100000]. It is running out of memory with that sizes\n",
    "sizes = [512,2048,4096,8192,16384,32768]\n",
    "threads = [1] #We put a single\n",
    "compares = []\n",
    "\n",
    "#The benchmark execute 5 times to gather data and afterwards \n",
    "for i in range(0,5):\n",
    "    print(\"Benchmark execution: \",i+1, \"\\n\")\n",
    "    compares.insert(i,benchMark(sizes,threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "different-smile",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-13T15:55:11.838984Z",
     "iopub.status.busy": "2021-04-13T15:55:11.835128Z",
     "iopub.status.idle": "2021-04-13T15:55:11.885582Z",
     "shell.execute_reply": "2021-04-13T15:55:11.885084Z"
    },
    "papermill": {
     "duration": 0.065959,
     "end_time": "2021-04-13T15:55:11.885709",
     "exception": false,
     "start_time": "2021-04-13T15:55:11.819750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Generate a file.out with the results.\n",
    "#Benchmark from pytorch just generate a print from the sdtout, so we need to change the stdout to write it in a file.\n",
    "import sys\n",
    "\n",
    "original_stdout = sys.stdout # Save a reference to the original standard output\n",
    "\n",
    "with open('output_gpu_benchmark.out', 'w') as file:\n",
    "    sys.stdout = file # Change the standard output to the file we created.\n",
    "    i=1\n",
    "    for compare in compares:\n",
    "        print(\"Benchmark execution: \",i, \"\\n\")\n",
    "        compare.print()\n",
    "        i += 1\n",
    "    sys.stdout = original_stdout # Reset the standard output to its original value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 107.663915,
   "end_time": "2021-04-13T15:55:14.182332",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-13T15:53:26.518417",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
