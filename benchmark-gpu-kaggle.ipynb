{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# Benchmarking GPUs in Kaggle\n\nIn this Kaggle notebook there are two examples of benchmarks GPU with pytorch. On one side it uses a model performance and on the other side it is an adaption from my previous CPU benchmark. ","metadata":{}},{"cell_type":"markdown","source":"## Benchmark GPU With models\nCode from project: https://github.com/ryujaehun/pytorch-gpu-benchmark\n\nRecollecting performance data of Kaggle.  **It must be activated the GPU accelerator option.**\n\nThe next block of code executes an extensive benchmark with DataSets from PyTorch on the GPU among differents types (type float, half and double). We are just going to use one type to simplify executions and tests.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\nimport platform\nimport psutil\nimport torch.nn as nn\nimport datetime\nimport time\nimport os\nimport pandas as pd\nimport argparse\nfrom torch.utils.data import Dataset, DataLoader\nimport json\ntorch.backends.cudnn.benchmark = True\n# https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936\n# This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware. \n# If you check it using the profile tool, the cnn method such as winograd, fft, etc. is used for the first iteration and the best operation is selected for the device.\n\n\n\nMODEL_LIST = {\n\n    models.mnasnet:models.mnasnet.__all__[1:],\n    models.resnet: models.resnet.__all__[1:],\n    models.densenet: models.densenet.__all__[1:],\n    models.squeezenet: models.squeezenet.__all__[1:],\n    models.vgg: models.vgg.__all__[1:],\n    models.mobilenet:models.mobilenet.__all__[1:],\n    models.shufflenetv2:models.shufflenetv2.__all__[1:]\n}\n#ORIGINAL\n#precisions=[\"float\",\"half\",'double']\nprecisions=[\"float\"]\n\n# For post-voltaic architectures, there is a possibility to use tensor-core at half precision.\n# Due to the gradient overflow problem, apex is recommended for practical use.\ndevice_name =str(torch.cuda.get_device_name(0))\n\n# Training settings\nparser = argparse.ArgumentParser(description='PyTorch Benchmarking')\nparser.add_argument('--WARM_UP','-w', type=int,default=5, required=False, help=\"Num of warm up\")\nparser.add_argument('--NUM_TEST','-n', type=int,default=50,required=False, help=\"Num of Test\")\nparser.add_argument('--BATCH_SIZE','-b', type=int, default=12, required=False, help='Num of batch size')\nparser.add_argument('--NUM_CLASSES','-c', type=int, default=1000, required=False, help='Num of class')\nparser.add_argument('--NUM_GPU','-g', type=int, default=1, required=False, help='Num of gpus')\nparser.add_argument('--folder','-f', type=str, default='result', required=False, help='folder to save results')\nargs = parser.parse_args()\nargs.BATCH_SIZE*=args.NUM_GPU\nclass RandomDataset(Dataset):\n\n    def __init__(self,  length):\n        self.len = length\n        self.data = torch.randn( 3, 224, 224,length)\n\n    def __getitem__(self, index):\n        return self.data[:,:,:,index]\n\n    def __len__(self):\n        return self.len\n\nrand_loader = DataLoader(dataset=RandomDataset( args.BATCH_SIZE*(args.WARM_UP + args.NUM_TEST)),\n                         batch_size=args.BATCH_SIZE, shuffle=False,num_workers=8)\ndef train(precision='single'):\n    \"\"\"use fake image for training speed test\"\"\"\n    target = torch.LongTensor(args.BATCH_SIZE).random_(args.NUM_CLASSES).cuda()\n    criterion = nn.CrossEntropyLoss()\n    benchmark = {}\n    for model_type in MODEL_LIST.keys():\n        for model_name in MODEL_LIST[model_type]:\n            model = getattr(model_type, model_name)(pretrained=False)\n            if args.NUM_GPU > 1:\n                model = nn.DataParallel(model,device_ids=range(args.NUM_GPU))\n            model=getattr(model,precision)()\n            model=model.to('cuda')\n            durations = []\n            print(f'Benchmarking Training {precision} precision type {model_name} ')\n            for step,img in enumerate(rand_loader):\n                img=getattr(img,precision)()\n                torch.cuda.synchronize()\n                start = time.time()\n                model.zero_grad()\n                prediction = model(img.to('cuda'))\n                loss = criterion(prediction, target)\n                loss.backward()\n                torch.cuda.synchronize()\n                end = time.time()\n                if step >= args.WARM_UP:\n                    durations.append((end - start)*1000)\n            print(f'{model_name} model average train time : {sum(durations)/len(durations)}ms')\n            del model\n            benchmark[model_name] = durations\n    return benchmark\n\ndef inference(precision='float'):\n    benchmark = {}\n    with torch.no_grad():\n        for model_type in MODEL_LIST.keys():\n            for model_name in MODEL_LIST[model_type]:\n                model = getattr(model_type, model_name)(pretrained=False)\n                if args.NUM_GPU > 1:\n                    model = nn.DataParallel(model,device_ids=range(args.NUM_GPU))\n                model=getattr(model,precision)()\n                model=model.to('cuda')\n                model.eval()\n                durations = []\n                print(f'Benchmarking Inference {precision} precision type {model_name} ')\n                for step,img in enumerate(rand_loader):\n                    img=getattr(img,precision)()\n                    torch.cuda.synchronize()\n                    start = time.time()\n                    model(img.to('cuda'))\n                    torch.cuda.synchronize()\n                    end = time.time()\n                    if step >= args.WARM_UP:\n                        durations.append((end - start)*1000)\n                print(f'{model_name} model average inference time : {sum(durations)/len(durations)}ms')\n                del model\n                benchmark[model_name] = durations\n    return benchmark\n\nf\"{platform.uname()}\\n{psutil.cpu_freq()}\\ncpu_count: {psutil.cpu_count()}\\nmemory_available: {psutil.virtual_memory().available}\"\n\n\nif __name__ == '__main__':\n    folder_name= \"../output/results\"\n    \n    device_name=f\"{device_name}_{args.NUM_GPU}_gpus_\"\n    system_configs=f\"{platform.uname()}\\n\\\n                     {psutil.cpu_freq()}\\n\\\n                    cpu_count: {psutil.cpu_count()}\\n\\\n                    memory_available: {psutil.virtual_memory().available}\"\n    gpu_configs=[torch.cuda.device_count(),torch.version.cuda,torch.backends.cudnn.version(),torch.cuda.get_device_name(0)]\n    gpu_configs=list(map(str,gpu_configs))\n    temp=['Number of GPUs on current device : ','CUDA Version : ','Cudnn Version : ','Device Name : ']\n\n    os.makedirs(folder_name, exist_ok=True)\n    with open(os.path.join(folder_name, 'config.json'), 'w') as f:\n        json.dump(vars(args), f, indent=2)\n    now = datetime.datetime.now()\n    \n    start_time=now.strftime('%Y/%m/%d %H:%M:%S')\n    \n    print(f'benchmark start : {start_time}')\n\n    for idx,value in enumerate(zip(temp,gpu_configs)):\n        gpu_configs[idx]=''.join(value)\n        print(gpu_configs[idx])\n    print(system_configs)\n\n    with open(os.path.join(folder_name,\"system_info.txt\"), \"w\") as f:\n        f.writelines(f'benchmark start : {start_time}\\n')\n        f.writelines('system_configs\\n\\n')\n        f.writelines(system_configs)\n        f.writelines('\\ngpu_configs\\n\\n')\n        f.writelines(s + '\\n' for s in gpu_configs )\n\n    \n    for precision in precisions:\n        train_result=train(precision)\n        train_result_df = pd.DataFrame(train_result)\n        path=f'{folder_name}/{device_name}_{precision}_model_train_benchmark.csv'\n        train_result_df.to_csv(path, index=False)\n\n        inference_result=inference(precision)\n        inference_result_df = pd.DataFrame(inference_result)\n        path=f'{folder_name}/{device_name}_{precision}_model_inference_benchmark.csv'\n        inference_result_df.to_csv(path, index=False)\n\n    now = datetime.datetime.now()\n\n    end_time=now.strftime('%Y/%m/%d %H:%M:%S')\n    print(f'benchmark end : {end_time}')\n    with open(os.path.join(folder_name,\"system_info.txt\"), \"a\") as f:\n        f.writelines(f'benchmark end : {end_time}\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os,time\nimport glob\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=True, world_readable=True)\nimport torchvision.models as models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_LIST = {\n    'mnasnet':models.mnasnet.__all__[1:],\n    'resnet': models.resnet.__all__[1:],\n    'densenet': models.densenet.__all__[1:],\n    'squeezenet': models.squeezenet.__all__[1:],\n    'vgg': models.vgg.__all__[1:],\n    'mobilenet':models.mobilenet.__all__[1:],\n    'shufflenetv2':models.shufflenetv2.__all__[1:]\n}\nfolder_name='../output/results/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(os.listdir(\"../output/results\"))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_list=glob.glob(folder_name+'/*.csv')\ncolumes=[]\nfor key,values in MODEL_LIST.items():\n    for i in values:\n        columes.append((key,i))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for csv in csv_list:\n    df=pd.read_csv(csv)\n    df.columns = pd.MultiIndex.from_tuples(columes)\n    df.groupby(level=0,axis=1).mean().mean()\n#     print(csv)\n    title=csv.split('/')[1].split('_benchmark')[0]\n    title=title.replace(' ','_')\n    df.groupby(level=0,axis=1).mean().mean().iplot(kind='scatter',mode='markers',title=title,yTitle='time(ms)',xTitle='models',asImage=True,filename=title)\n    for model in MODEL_LIST.keys():\n        df.mean()[model].iplot(kind='scatter',mode='markers',title=model+\"_\"+title,yTitle='time(ms)',xTitle='models',asImage=True,filename=model+\"_\"+title)\n        time.sleep(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## My CPU Benchmark adapted to GPU\n\nPytorch has hard coded a block size of 512 threads. So there's only one execution per matrix size.\n\nAs last benchmark, GPU accelerator has to be activated to use this benchmark.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#Author: Raquel Ricoy\n\n#Benchmark to study Kaggle's GPUs, CPUs and TPUs potential.\n#It's going to use Pytorch and to stablish a script to calculate its performance and GFLOPS.\n\n#Install pytorch\n#!conda install -y pytorch torchvision -c pytorch\n\nimport torch\nimport platform\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\n#print(os.listdir(\"../input\"))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n#Importing Libraries needed for use torch\nimport timeit\nimport torch.utils.benchmark as benchmark\nfrom itertools import product","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Functions obtained from Torch Webpages por PyTorch Benchmarks\ndef batched_dot_mul_sum(a, b):\n    '''Computes batched dot by multiplying and summing'''\n    return a.mul(b).sum(-1)\n\n\ndef batched_dot_bmm(a, b):\n    '''Computes batched dot by reducing to bmm'''\n    a = a.reshape(-1, 1, a.shape[-1])\n    b = b.reshape(-1, b.shape[-1], 1)\n    return torch.bmm(a, b).flatten(-3)\n","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Method that do the benchmark and compare results with dot mul sum implementations and vectorSum\ndef benchMark(sizes,nThreads):\n    results = []\n    if(len(sizes) == 0):\n        print(\"Parameter 'sizes' has to a have minumun of 1 parameters\")\n        return\n    if(len(nThreads)==0):\n        print(\"Parameter 'nThreads' has to a have minumun of 1 parameters\")\n    \n    for n in sizes:\n        # label and sub_label are the rows\n        # description is the column\n        label = 'Batched dot'\n        sub_label = f'[{n}, {n}]'\n        x = torch.ones((n, n),device='cuda')\n        for num_threads in nThreads:\n            results.append(benchmark.Timer(\n                stmt='batched_dot_mul_sum(x, x)',\n                setup='from __main__ import batched_dot_mul_sum',\n                globals={'x': x},\n                num_threads=num_threads,\n                label=label,\n                sub_label=sub_label,\n                description='mul/sum',\n            ).blocked_autorange(min_run_time=1))\n            results.append(benchmark.Timer(\n                stmt='batched_dot_bmm(x, x)',\n                setup='from __main__ import batched_dot_bmm',\n                globals={'x': x},\n                num_threads=num_threads,\n                label=label,\n                sub_label=sub_label,\n                description='bmm',\n            ).blocked_autorange(min_run_time=1))\n    compare = benchmark.Compare(results)\n    compare.print()","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#The limit dimension of the sizes is [100000,100000]. It is running out of memory with that sizes\nsizes = [512,2048,4096,8192,16384,32768]\nthreads = [1] #We put a single\n\nbenchMark(sizes,threads)","metadata":{"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-3ee7f93df11d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mthreads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#We put a single\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbenchMark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-f350871c6573>\u001b[0m in \u001b[0;36mbenchMark\u001b[0;34m(sizes, nThreads)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Batched dot'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0msub_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'[{n}, {n}]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnum_threads\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnThreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             results.append(benchmark.Timer(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise AssertionError(\n","\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"],"ename":"AssertionError","evalue":"Torch not compiled with CUDA enabled","output_type":"error"}]}]}