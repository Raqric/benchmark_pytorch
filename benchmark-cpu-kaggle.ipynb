{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#Author: Raquel Ricoy\n\n#Benchmark to study Kaggle's GPUs, CPUs and TPUs potential.\n#It's going to use Pytorch and to stablish a script to calculate its performance and GFLOPS.\n\n#Install pytorch\n#!conda install -y pytorch torchvision -c pytorch\n\nimport torch\nimport platform\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nprint(os.listdir(\"../input\"))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n#Importing Libraries needed for use torch\nimport timeit\nimport torch.utils.benchmark as benchmark\nfrom itertools import product","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[]\n","output_type":"stream"}]},{"cell_type":"code","source":"#Information about system\nprint('Platform processor:', platform.processor())\nprint('Platform architecture:', platform.architecture())\n\n#Number of threads\nnum_cores = os.cpu_count()\nprint('Number of cores:',num_cores)\ntorch.set_num_threads(num_cores)\nnum_threads = num_cores","metadata":{"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Platform processor: x86_64\nPlatform architecture: ('64bit', '')\nNumber of cores: 4\n","output_type":"stream"}]},{"cell_type":"code","source":"#Functions obtained from Torch Webpages por PyTorch Benchmarks\ndef batched_dot_mul_sum(a, b):\n    '''Computes batched dot by multiplying and summing'''\n    return a.mul(b).sum(-1)\n\n\ndef batched_dot_bmm(a, b):\n    '''Computes batched dot by reducing to bmm'''\n    a = a.reshape(-1, 1, a.shape[-1])\n    b = b.reshape(-1, b.shape[-1], 1)\n    return torch.bmm(a, b).flatten(-3)\n\n#Function developed by my own. Sum two vectors and save the output in vector C\ndef sumVector(aVector,bVector):\n    lengthVectors = len(aVector);\n    cVector = torch.empty(lengthVectors,dtype=torch.float)\n    for i in torch.arange(0,lengthVectors):\n        cVector[i] = aVector[i] + bVector[i]\n    return cVector","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Method that do the benchmark and compare results with dot mul sum implementations and vectorSum\ndef benchMark(sizes,nThreads):\n    results = []\n    if(len(sizes) == 1):\n        print(\"Parameter 'sizes' has to a have minumun of two parameters\")\n        return\n    if(len(nThreads)==0):\n        print(\"Parameter 'nThreads' has to a have minumun of two parameters\")\n    \n    for b, n in product(sizes, sizes):\n        # label and sub_label are the rows\n        # description is the column\n        label = 'Batched dot'\n        sub_label = f'[{b}, {n}]'\n        x = torch.ones((b, n))\n        for num_threads in nThreads:\n            results.append(benchmark.Timer(\n                stmt='batched_dot_mul_sum(x, x)',\n                setup='from __main__ import batched_dot_mul_sum',\n                globals={'x': x},\n                num_threads=num_threads,\n                label=label,\n                sub_label=sub_label,\n                description='mul/sum',\n            ).blocked_autorange(min_run_time=1))\n            results.append(benchmark.Timer(\n                stmt='batched_dot_bmm(x, x)',\n                setup='from __main__ import batched_dot_bmm',\n                globals={'x': x},\n                num_threads=num_threads,\n                label=label,\n                sub_label=sub_label,\n                description='bmm',\n            ).blocked_autorange(min_run_time=1))\n    compare = benchmark.Compare(results)\n    compare.print()","metadata":{"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#The limit dimension of the sizes is [100000,100000]. It is running out of memory with that sizes\nsizes = [1, 64, 1024, 2048,4096]\nthreads = range(1,num_threads+1)\n\n#Este metodo no vale para hacer benchmark de gpus, habria que tunearlo\nbenchMark(sizes,threads)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}