{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#Author: Raquel Ricoy\n\n#Benchmark to study Kaggle's GPUs, CPUs and TPUs potential.\n#It's going to use Pytorch and to stablish a script to calculate its performance and GFLOPS.\n\n#Install pytorch\n#!conda install -y pytorch torchvision -c pytorch\n\nimport torch\nimport platform\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\n#print(os.listdir(\"../input\"))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n#Importing Libraries needed for use torch\nimport timeit\nimport torch.utils.benchmark as benchmark\nfrom itertools import product","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#Information about system\nprint('Platform processor:', platform.processor())\nprint('Platform architecture:', platform.architecture())\n\n#Number of threads\nnum_cores = os.cpu_count()\nprint('Number of cores:',num_cores)\ntorch.set_num_threads(num_cores)\nnum_threads = num_cores","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Platform processor: x86_64\nPlatform architecture: ('64bit', '')\nNumber of cores: 4\n","output_type":"stream"}]},{"cell_type":"code","source":"#Functions obtained from Torch Webpages por PyTorch Benchmarks\ndef batched_dot_mul_sum(a, b):\n    '''Computes batched dot by multiplying and summing'''\n    return a.mul(b).sum(-1)\n\n\ndef batched_dot_bmm(a, b):\n    '''Computes batched dot by reducing to bmm'''\n    a = a.reshape(-1, 1, a.shape[-1])\n    b = b.reshape(-1, b.shape[-1], 1)\n    return torch.bmm(a, b).flatten(-3)\n\n#Function developed by my own. Sum two vectors and save the output in vector C\ndef sumVector(aVector,bVector):\n    lengthVectors = len(aVector);\n    cVector = torch.empty(lengthVectors,dtype=torch.float)\n    for i in torch.arange(0,lengthVectors):\n        cVector[i] = aVector[i] + bVector[i]\n    return cVector","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Method that do the benchmark and compare results with dot mul sum implementations and vectorSum\ndef benchMark(sizes,nThreads):\n    results = []\n    if(len(sizes) == 0):\n        print(\"Parameter 'sizes' has to a have minumun of 1 parameters\")\n        return\n    if(len(nThreads)==0):\n        print(\"Parameter 'nThreads' has to a have minumun of 1 parameters\")\n    \n    for n in sizes:\n        # label and sub_label are the rows\n        # description is the column\n        label = 'Batched dot'\n        sub_label = f'[{n}, {n}]'\n        x = torch.ones((n, n))\n        for num_threads in nThreads:\n            results.append(benchmark.Timer(\n                stmt='batched_dot_mul_sum(x, x)',\n                setup='from __main__ import batched_dot_mul_sum',\n                globals={'x': x},\n                num_threads=num_threads,\n                label=label,\n                sub_label=sub_label,\n                description='mul/sum',\n            ).blocked_autorange(min_run_time=1))\n            results.append(benchmark.Timer(\n                stmt='batched_dot_bmm(x, x)',\n                setup='from __main__ import batched_dot_bmm',\n                globals={'x': x},\n                num_threads=num_threads,\n                label=label,\n                sub_label=sub_label,\n                description='bmm',\n            ).blocked_autorange(min_run_time=1))\n    compare = benchmark.Compare(results)\n    compare.print()","metadata":{"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"#The limit dimension of the sizes is [100000,100000]. It is running out of memory with that sizes\nsizes = [512,1024,2048,4096,5120]\nthreads = range(1,num_threads+1)\n\n#Este metodo no vale para hacer benchmark de gpus, habria que tunearlo\nbenchMark(sizes,threads)","metadata":{"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"[------------- Batched dot --------------]\n                    |  mul/sum  |    bmm  \n1 threads: -------------------------------\n      [512, 512]    |    139.9  |    821.8\n      [1024, 1024]  |    606.3  |   3194.9\n      [2048, 2048]  |   4427.8  |  12577.0\n      [4096, 4096]  |  19489.2  |  49718.9\n      [5120, 5120]  |  29783.5  |  77762.1\n2 threads: -------------------------------\n      [512, 512]    |     63.6  |    431.4\n      [1024, 1024]  |    320.5  |   1625.0\n      [2048, 2048]  |   1988.8  |   6451.8\n      [4096, 4096]  |  10112.6  |  25135.2\n      [5120, 5120]  |  15452.0  |  39218.4\n3 threads: -------------------------------\n      [512, 512]    |    101.4  |    387.5\n      [1024, 1024]  |    426.7  |   1386.2\n      [2048, 2048]  |   2487.1  |   5538.8\n      [4096, 4096]  |  12022.9  |  20877.9\n      [5120, 5120]  |  18719.1  |  32533.7\n4 threads: -------------------------------\n      [512, 512]    |     68.3  |    299.8\n      [1024, 1024]  |    317.2  |   1060.1\n      [2048, 2048]  |   1662.1  |   4105.6\n      [4096, 4096]  |   8917.8  |  15970.0\n      [5120, 5120]  |  13687.2  |  24462.7\n\nTimes are in microseconds (us).\n\n","output_type":"stream"}]}]}